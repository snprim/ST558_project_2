---
output: 
  github_document: 
    toc: true
params: 
  weekday: Monday
editor_options: 
  chunk_output_type: inline
---

---
title: `r params$weekday`  
author: "Shih-Ni Prim"  
date: "2020-10-16"  
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
```

```{r, echo = FALSE, eval = FALSE}
# dayz <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
# paramz <- lapply(dayz, FUN = function(x){list(weekday = x)})
# output_file <- paste0("Report-", dayz, ".md")
# reports <- tibble(output_file, paramz)
# 
# library(rmarkdown)
# 
# apply(reports, MARGIN = 1,
#       FUN = function(x){
#           render(input = "analysis.Rmd",
#                  output_format = "github_document",
#                  output_file = x[[1]],
#                  params = x[[2]])
#           })

rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Sunday.md", params = list(weekday = "Sunday"))
rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Monday.md", params = list(weekday = "Monday"))
rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Tuesday.md", params = list(weekday = "Tuesday"))
rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Wednesday.md", params = list(weekday = "Wednesday"))
rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Thursday.md", params = list(weekday = "Thursday"))
rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Friday.md", params = list(weekday = "Friday"))
rmarkdown::render(input = "analysis.Rmd", output_format = "github_document", output_file = "Report-Saturday.md", params = list(weekday = "Saturday"))
```

## Introduction  

Now we take a look at `r params$weekday`'s analysis. This dataset contains information about [bike sharing](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). We have a variety of predictors, including hours, temperature, humidity, weekday, holiday/workday or not, etc. In our analysis, We will use two statistical learning models--regression tree and boosted tree--to predict the count of total rental bikes `cnt`.  

## Setting the Value for the Parameter

Since the current analysis is on `r params$weekday`, we first find the corresponding value for it.  

```{r}
set.seed(7777)
i <- 0:6
dayz <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
df <- as.data.frame(cbind(i, dayz))
weekdayNum <- df$i[df$dayz == params$weekday]
print(weekdayNum)
```

## Data  

Now we read in the data. Two datasets are listed on [the link](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset), one including the `hr` variable, and one treating each day as one observation and thus not including the `hr` variable. Since hours--the time in the day--should be a meaningful predictor for the number of bike rentals, we use the dataset with the `hr` variable  

```{r}
bikes <- read_csv("../Bike-Sharing-Dataset/hour.csv")
# head(bikes)
analysis <- bikes %>% filter(weekday == weekdayNum) %>% select(-casual, -registered) %>% select(dteday, weekday, everything()) 
# head(analysis)
```

## Splitting Data  

We first split up the data into two sets: training and test sets. The training set has about 70% of the data, and the test set has about 30%. Splitting up the data is important, because we want to test the model on a set that is not used in training, otherwise we risk overfitting.  

```{r}
train <- sample(1:nrow(analysis), size = nrow(analysis)*0.7)
test <- setdiff(1:nrow(analysis), train)

bikeTrain <- analysis[train,]
bikeTest <- analysis[test,]
```

## Summaries and Exploratory Data Analysis

To decide which variables to include in our models, we first take a quick look at the data. We can look at summaries of numerical variables.  

```{r}
summary(bikeTrain)
```

Below we look at three plots. The first plot shows the histogram of bike rentals (`cnt`) on `r params$weekday`. The second plot shows that `cnt` does vary in different hours. The third plot shows that `cnt` varies between the two years. So we know we should keep `hr` and `yr` as predictors.  

```{r}
ggplot(bikeTrain, mapping = aes(x = cnt)) + geom_histogram()
ggplot(bikeTrain, aes(x = hr, y = cnt)) + geom_point() + geom_jitter()
ggplot(bikeTrain, aes(x = yr, y = cnt)) + geom_boxplot(aes(group = yr))
```

Next we look at correlations of different variables. Weather and windspeed do not seem correlate, so we will keep both `weathersit` and `windspeed`. 

```{r}
ggplot(bikeTrain, aes(x = weathersit, y = windspeed)) + geom_jitter()
```

Several pairs of variables seem highly correlated--`season` and `mnth`, `holiday` and `workingday`--so we'll remove one from each pair. 

```{r}
cor(bikeTrain$season, bikeTrain$mnth)
cor(bikeTrain$holiday, bikeTrain$workingday)
cor(bikeTrain$temp, bikeTrain$atemp)
```

The variance of `workingday` and `holiday` are too small and probably not good predictors.  

```{r}
var(bikeTrain$holiday)
var(bikeTrain$workingday)
```

Also, `instant` and `dteday` are for record-keeping. Thus, we decide to keep the following variables as the predictors: `season`, `yr`, `hr`, `weathersit`, `atemp`, `hum`, and `windspeed`.  

```{r}
bikeTrain <- select(bikeTrain, season, yr, hr, weathersit, atemp, hum, windspeed, cnt)
bikeTest <- select(bikeTest, season, yr, hr, weathersit, atemp, hum, windspeed, cnt)
```

## Fitting models  

Now we have a final training set and have chosen the predictors, we can use two models--regression tree and boosted tree--to fit the training data.  

### Regression tree  

A regression tree is one of the tree based methods for supervised learning with the goal of predicting a continuous response. It splits up predictor space into different regions, and the prediction of each region is often the mean of observations in that region.  

For regression tree, we use the `caret` package and apply the leave-one-out cross validation method (thus the argument `method = "LOOCV"`). We set the `tuneLength` as 10 and let the model chooses the best model automatically.  

```{r, cache = TRUE}
modelLookup("rpart")

bikeTree <- train(cnt ~ ., data = bikeTrain, method = "rpart", trControl = trainControl(method = "LOOCV"), tuneGrid = expand.grid(cp = seq(0.01, 0.02, 0.001)))
```

Below we can see the final model; the resulting RMSE, Rsquared, and MAE of different cp; and a plot that shows the relationship between cp and RMSE.  

```{r}
bikeTree$finalModel
bikeTree
plot(bikeTree)
```

Finally we use the model to predict `cnt` on the test data and calculate RMSE to check the fit of the model.  

```{r}
predTree <- predict(bikeTree, newdata = bikeTest)
treeResult <- postResample(predTree, bikeTest$cnt)
```


### Boosted Tree  

A boosted tree is one of the ensemble learning methods, in which the tree grows sequentially. Each subsequent tree is combined into the previous model to produce a modified model. The predictions are updated as the tree grows.  

We again use `caret` package and set the method as `gbm`. We use repeated cross validation (`repeatedcv`) and set the `tuneLength` as 10 and let the model chooses the best model automatically.  

```{r, cache = TRUE}
modelLookup("gbm")

grid <- expand.grid(n.trees = c(50, 100, 150), interaction.depth = 1:4, shrinkage = c(0.1, 0.01), n.minobsinnode = c(10, 15, 20))

boostedBike <- train(cnt ~  season + yr + hr + weathersit + atemp + hum + windspeed, data = bikeTrain, method = "gbm", preProcess = c("center", "scale"), trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3), tuneGrid = grid, verbose = FALSE)
```

Below we can see some information about the final model, the predictors chosen and their importance, and a plot that shows how RMSE changes with different numbers of boosting iterations and tree depths.  


```{r}
boostedBike$finalModel
summary(boostedBike)
plot(boostedBike)
```

Finally, we use the model to predict `cnt` on the test data and calculate RMSE to check the fit of the model.  

```{r}
predBoostedBike <- predict(boostedBike, newdata = select(bikeTest, -cnt))
boostedResult <- postResample(predBoostedBike, bikeTest$cnt)
```


### Comparison  

We can put the testing RMSE from the two models together for comparison.  

```{r}
comparison <- data.frame(rbind(t(treeResult), t(boostedResult)))
colnames(comparison) <- c("RMSE", "Rsquared", "MAE")
rownames(comparison) <- c("Regression Tree", "Boosted Tree")
knitr::kable(comparison)
```

### Final Model  

```{r}
# a function to generate the name of the best model
model <- function(x, y){
  xscore <- 0
  if (x[[1]] < y[[1]]) {
    xscore = xscore + 1
  }
  if (x[[2]] > y[[2]]){
    xscore = xscore + 1
  }
  if (x[[3]] < y[[3]]){
    xscore = xscore + 1
  }
  if (xscore == 2 || xscore == 3){
    final <- c("regression tree")
  } else {
    final <- c("boosted tree")
  }
  return(final)
}
# model(treeResult, boostedResult)
```

From the output, we can conclude that the `r model(treeResult, boostedResult)` is the better model for `r params$weekday` data, because it has better performance in terms of RMSE, Rsquared, and MAE.  

